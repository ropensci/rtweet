---
title: "Live streaming tweets"
subtitle: "rtweet: Collecting Twitter Data"
output: 
  rmarkdown::html_vignette:
    self_contained: false
vignette: >
  %\VignetteIndexEntry{Live streaming tweets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, comment = "#>", collapse = TRUE,
                      fig.path = "vignettes/files/", root.dir = "vignettes")
```

## Installing and loading package

Prior to streaming, make sure to install and load rtweet. 
This vignette assumes users have already setup user access tokens (see: the "auth" vignette, `vignette("auth", package = "rtweet")`).

```{r library} 
## Load rtweet
library(rtweet)
```

```{r auth, include=FALSE, purl=FALSE}
auth_as(rtweet:::rtweet_test())
```

## Specifying parameters

In addition to accessing Twitter's REST API (e.g., `search_tweets`, `get_timeline`), rtweet makes it possible to capture live streams of Twitter data using the `stream_tweets()` function. 
By default, `stream_tweets` will stream for 30 seconds and return a random sample of tweets. 
To modify the default settings, `stream_tweets` accepts several parameters, including `q` (query used to filter tweets), `timeout` (duration or time of stream), and `file_name` (path name for saving raw json data).

```{r stream-setup}
## Stream keywords used to filter tweets
q <- "#rstats"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 5 seconds
streamtime <- 5 
## Filename to save json data (backup)
filename <- "rstats.json"
```

## stream_tweets()

Once these parameters are specified, initiate the stream. 
Note: Barring any disconnection or disruption of the API, streaming will occupy your current instance of R until the specified time has elapsed. 
It is possible to start a new instance or R ---streaming itself usually isn't very memory intensive--- but operations may drag a bit during the parsing process which takes place immediately after streaming ends.

```{r stream-rstats}
## Stream election tweets
stream_rstats <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```

Parsing larger streams can take quite a bit of time (in addition to time spent streaming) due to a somewhat time-consuming simplifying process used to convert a json file into an R object. 
It is recommended to save the streaming in a file.

## Saving files

Given a lengthy parsing process, users may want to stream tweets into json files upfront and parse those files later on. 
To do this, simply add `parse = FALSE` and make sure you provide a path (file name) to a location you can find later. 

Regardless of whether you decide to setup a system for streaming data, the process of streaming a file to disk and parsing it at a later point in time is the same, as illustrated in the example below.

```{r stream-append}
## No upfront-parse save as json file instead method
# By default results are appended to the file.
stream_tweets(
  q = q,
  parse = FALSE,
  timeout = streamtime,
  file_name = filename
)

## Parse from json file
stream_rstats <- parse_stream(filename)
```

## Returned data object

The parsed object should be the same whether a user parses up-front or from a json file in a later session. 
The returned object should be a data frame consisting of tweets data.

```{r stream-tweets}
## Preview tweets data
head(stream_rstats)
```

The returned object should also include a data frame of users data, which Twitter's stream API automatically returns along with tweets data. 
To access users data, use the `users_data` function.

```{r users_data}
## Preview users data
head(users_data(stream_rstats))
```

## Plotting

Once parsed, `ts_plot()` provides a quick visual of the frequency of tweets. 
By default, `ts_plot()` will try to aggregate time by the day. 
Because I know the stream only lasted 30 minutes, I've opted to aggregate tweets by the second. 
It'd also be possible to aggregate by the minute, i.e., `by = "mins"`, or by some value of seconds, e.g.,`by = "15 secs"`. 

```{r ts_plot}
## Plot time series of all tweets aggregated by second
ts_plot(stream_rstats, by = "secs")
```
